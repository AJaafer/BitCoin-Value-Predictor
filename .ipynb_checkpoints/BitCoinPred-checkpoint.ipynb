{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark as spark\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import col,udf,monotonically_increasing_id,unix_timestamp,round,avg\n",
    "import re\n",
    "sc = spark.SparkContext()\n",
    "sql = spark.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping line 845142: unexpected end of data\n"
     ]
    }
   ],
   "source": [
    "df1=pd.read_csv('/home/manoj/ProjectBigData/Data/tweetsfinal.csv',error_bad_lines=False,engine = 'python',header = None) \n",
    "df2=pd.read_csv('/home/manoj/ProjectBigData/Data/BitCoinPrice.csv',error_bad_lines=False,engine = 'python',header = None) \n",
    "FullDataTw=sql.createDataFrame(df1)\n",
    "FullDataBtc=sql.createDataFrame(df2) #creating pandas df and then changing it to pyspark df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "845141\n",
      "217\n"
     ]
    }
   ],
   "source": [
    "FullDataTw = FullDataTw.dropna() #getting rid of full empty rows\n",
    "print(FullDataTw.count())\n",
    "print(FullDataBtc.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FullDataTw.select(monotonically_increasing_id().alias(\"rowId\"),\"*\")\n",
    "FullDataTw = FullDataTw.withColumnRenamed('0', 'DateTime') #setting column names of Twitter dataset\n",
    "FullDataTw = FullDataTw.withColumnRenamed('1', 'Tweet')\n",
    "FullDataBtc = FullDataBtc.withColumnRenamed('0', 'DateTime') #setting column names of Bitcoin price dataset\n",
    "FullDataBtc = FullDataBtc.withColumnRenamed('1', 'Price')\n",
    "FullDataBtc = FullDataBtc.filter(FullDataBtc.DateTime != 'Date') #to get rid of first row with the header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Tw_samp = FullDataTw #taking sample of 50 rows and working on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|            DateTime|               Tweet|       CleanedTweets|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|Thu Nov 09 17:43:...|RT @Forbes: The F...|The Failure of Se...|\n",
      "|Thu Nov 09 17:43:...|RT @mindstatex: L...|Lots of love from...|\n",
      "|Thu Nov 09 17:43:...|RT @FernandoHuama...|Warning Built in ...|\n",
      "|Thu Nov 09 17:43:...|RT @LevelNetwork:...|Join our telegram...|\n",
      "|Thu Nov 09 17:43:...|RT @realsheepwolf...|DIGAF FLOAT 16M T...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import preprocessor as p #cleaning each tweet using tweet-preprocessor like removing hashtags,urls,emojis....\n",
    "def function_udf(input_str):\n",
    "    input_str = re.sub(r'RT', '', input_str)\n",
    "    p.set_options(p.OPT.URL, p.OPT.EMOJI,p.OPT.MENTION)\n",
    "    input_str = p.clean(input_str)\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", input_str).split())\n",
    "func_udf = udf(function_udf, StringType())\n",
    "CleanDF = Tw_samp.withColumn('CleanedTweets', func_udf(Tw_samp['Tweet']))\n",
    "CleanDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+---------------+\n",
      "|            DateTime|               Tweet|       CleanedTweets|Sentiment_score|\n",
      "+--------------------+--------------------+--------------------+---------------+\n",
      "|Thu Nov 09 17:43:...|RT @Forbes: The F...|The Failure of Se...|    -0.18888889|\n",
      "|Thu Nov 09 17:43:...|RT @mindstatex: L...|Lots of love from...|     0.25833333|\n",
      "|Thu Nov 09 17:43:...|RT @FernandoHuama...|Warning Built in ...|            0.0|\n",
      "|Thu Nov 09 17:43:...|RT @LevelNetwork:...|Join our telegram...|            0.0|\n",
      "|Thu Nov 09 17:43:...|RT @realsheepwolf...|DIGAF FLOAT 16M T...|          -0.05|\n",
      "+--------------------+--------------------+--------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob  #passing cleaned tweets and getting a sentiment score for each tweet\n",
    "def senti_score_udf(input_str):\n",
    "    analysis = TextBlob(input_str)\n",
    "    return analysis.sentiment.polarity\n",
    "func_udf2 = udf(senti_score_udf, FloatType())\n",
    "CleanDF = CleanDF.withColumn('Sentiment_score', func_udf2(CleanDF['CleanedTweets']))\n",
    "CleanDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+---------------+-------------------+-------------------+\n",
      "|            DateTime|               Tweet|       CleanedTweets|Sentiment_score|         DateTime_c|    DateTime_casted|\n",
      "+--------------------+--------------------+--------------------+---------------+-------------------+-------------------+\n",
      "|Thu Nov 09 17:43:...|RT @Forbes: The F...|The Failure of Se...|    -0.18888889|2017-11-09 17:43:41|2017-11-09 17:43:41|\n",
      "|Thu Nov 09 17:43:...|RT @mindstatex: L...|Lots of love from...|     0.25833333|2017-11-09 17:43:40|2017-11-09 17:43:40|\n",
      "|Thu Nov 09 17:43:...|RT @FernandoHuama...|Warning Built in ...|            0.0|2017-11-09 17:43:39|2017-11-09 17:43:39|\n",
      "|Thu Nov 09 17:43:...|RT @LevelNetwork:...|Join our telegram...|            0.0|2017-11-09 17:43:39|2017-11-09 17:43:39|\n",
      "|Thu Nov 09 17:43:...|RT @realsheepwolf...|DIGAF FLOAT 16M T...|          -0.05|2017-11-09 17:43:39|2017-11-09 17:43:39|\n",
      "+--------------------+--------------------+--------------------+---------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def Tw_Time_format(stri):  #manipulating and casting the strings(DateTime) of tweets dataframe to timestamps\n",
    "    dic = {'Nov':'11','Oct':'10'}\n",
    "    ans = ''\n",
    "    ans += stri[-4:]+'-'+ dic[stri[4:7]]+'-'+stri[8:19]\n",
    "    return ans\n",
    "func_udf3 = udf(Tw_Time_format,StringType())\n",
    "CleanDF = CleanDF.withColumn('DateTime_c', func_udf3(CleanDF['DateTime']))\n",
    "CleanDF = CleanDF.withColumn(\"DateTime_casted\",CleanDF['DateTime_c'].cast(TimestampType()))\n",
    "CleanDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+---------------+\n",
      "|          Date_Time|      Cleaned_Tweets|Sentiment_score|\n",
      "+-------------------+--------------------+---------------+\n",
      "|2017-11-09 17:43:41|The Failure of Se...|    -0.18888889|\n",
      "|2017-11-09 17:43:40|Lots of love from...|     0.25833333|\n",
      "|2017-11-09 17:43:39|Warning Built in ...|            0.0|\n",
      "|2017-11-09 17:43:39|Join our telegram...|            0.0|\n",
      "|2017-11-09 17:43:39|DIGAF FLOAT 16M T...|          -0.05|\n",
      "+-------------------+--------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FinalTw = CleanDF.selectExpr(\"DateTime_casted as Date_Time\", \"CleanedTweets as Cleaned_Tweets\",\"Sentiment_score\")\n",
    "FinalTw.show(5) #selecting necessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date_Time: timestamp (nullable = true)\n",
      " |-- Cleaned_Tweets: string (nullable = true)\n",
      " |-- Sentiment_score: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FinalTw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime \n",
    "from dateutil import parser\n",
    "def Btc_Time_format(input_str): #manipulating and casting the strings(DateTime) of BTC dataframe to timestamps\n",
    "    input_str = re.sub(r'/17','', input_str)\n",
    "    input_str = '2017-'+ input_str\n",
    "    input_str = re.sub(r'/', '-', input_str)\n",
    "    input_str += ':00'\n",
    "    return input_str[:10]+\"\"+input_str[10:]\n",
    "func_udf = udf(Btc_Time_format, StringType())\n",
    "FullDataBtc = FullDataBtc.withColumn('Cleaned_BTC_Time', func_udf(FullDataBtc['DateTime']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+\n",
      "|          Date_Time|  Price|\n",
      "+-------------------+-------+\n",
      "|2017-10-31 00:00:00|6142.46|\n",
      "|2017-10-31 01:00:00|6139.47|\n",
      "|2017-10-31 02:00:00| 6128.2|\n",
      "|2017-10-31 03:00:00|6130.72|\n",
      "|2017-10-31 04:00:00|6143.92|\n",
      "+-------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CleandfBtc = FullDataBtc.withColumn(\"Cleaned_BTC_Time_New\",FullDataBtc['Cleaned_BTC_Time'].cast(TimestampType()))\n",
    "FinalBtc = CleandfBtc.selectExpr(\"Cleaned_BTC_Time_New as Date_Time\", \"Price\")\n",
    "FinalBtc = FinalBtc.withColumn(\"Price\",FinalBtc['Price'].cast(DoubleType()))\n",
    "FinalBtc.show(5)#In this cell, casting to timesstamp, changing col names and casting price type to double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date_Time: timestamp (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FinalBtc.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+---------------+\n",
      "|          Date_Time|      Cleaned_Tweets|Sentiment_score|\n",
      "+-------------------+--------------------+---------------+\n",
      "|2017-11-09 23:00:00|The Failure of Se...|    -0.18888889|\n",
      "|2017-11-09 23:00:00|Lots of love from...|     0.25833333|\n",
      "|2017-11-09 23:00:00|Warning Built in ...|            0.0|\n",
      "|2017-11-09 23:00:00|Join our telegram...|            0.0|\n",
      "|2017-11-09 23:00:00|DIGAF FLOAT 16M T...|          -0.05|\n",
      "|2017-11-09 23:00:00|My luggage likes ...|            0.0|\n",
      "|2017-11-09 23:00:00|As Bitcoin become...|           0.55|\n",
      "|2017-11-09 23:00:00|A crucial feature...|            0.1|\n",
      "|2017-11-09 23:00:00|As Bitcoin become...|           0.55|\n",
      "|2017-11-09 23:00:00|As Bitcoin become...|           0.55|\n",
      "+-------------------+--------------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_truncated = ((round(unix_timestamp(col('Date_Time')) / 3600) * 3600).cast('timestamp'))\n",
    "FinalTw = FinalTw.withColumn('dt_truncated', dt_truncated)\n",
    "FinalTw = FinalTw.selectExpr(\"dt_truncated as Date_Time\",\"Cleaned_Tweets\",\"Sentiment_score\")\n",
    "UTC = ((unix_timestamp(col('Date_Time'))+ 5*60*60).cast('timestamp'))\n",
    "FinalTw = FinalTw.withColumn('UTC', UTC)\n",
    "FinalTw = FinalTw.selectExpr(\"UTC as Date_Time\",\"Cleaned_Tweets\",\"Sentiment_score\")\n",
    "FinalTw.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|          Date_Time|avg(Sentiment_score)|\n",
      "+-------------------+--------------------+\n",
      "|2017-11-09 09:00:00| 0.09240741625240456|\n",
      "|2017-11-04 14:00:00| 0.11845594881100904|\n",
      "|2017-11-04 13:00:00|  0.1172484553339806|\n",
      "|2017-11-02 12:00:00| 0.06527885267149504|\n",
      "|2017-11-07 23:00:00| 0.10283782658623176|\n",
      "|2017-11-04 09:00:00| 0.09418897322089734|\n",
      "|2017-11-02 02:00:00| 0.09436981377657502|\n",
      "|2017-11-03 03:00:00| 0.09344412833299827|\n",
      "|2017-11-04 04:00:00| 0.10142201882228671|\n",
      "|2017-11-05 00:00:00| 0.08632902932326081|\n",
      "|2017-10-31 17:00:00| 0.09206832917391784|\n",
      "|2017-11-06 16:00:00| 0.09368665402965974|\n",
      "|2017-11-03 05:00:00| 0.07860593016294816|\n",
      "|2017-11-02 21:00:00| 0.08349927669926008|\n",
      "|2017-11-06 19:00:00| 0.10290178138846101|\n",
      "|2017-11-02 16:00:00| 0.06061797001144715|\n",
      "|2017-11-01 05:00:00| 0.11368575691864594|\n",
      "|2017-10-31 16:00:00| 0.09301943334583278|\n",
      "|2017-10-31 12:00:00| 0.10549754665746006|\n",
      "|2017-11-04 19:00:00| 0.09140639081231433|\n",
      "+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- Date_Time: timestamp (nullable = true)\n",
      " |-- avg(Sentiment_score): double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FinalTw_avg = FinalTw.select(\"Date_Time\",\"Sentiment_score\").groupBy(\"Date_Time\").agg(avg(col(\"Sentiment_score\")))\n",
    "# FinalTw_avg.show()\n",
    "# FinalTw_avg.printSchema()\n",
    "FinalTw_avg = FinalTw_avg.selectExpr(\"Date_Time as date\", \"`avg(Sentiment_score)` as score\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- score: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FinalTw_avg.printSchema()\n",
    "\n",
    "# FinalTw.registerTempTable(\"temp\")\n",
    "# FinalTw_avg = sql.sql(\"SELECT Date_Time As DateTime, AVG(Sentiment_score) As Sentiment_score FROM temp GROUP BY Date_Time\")\n",
    "# FinalTw_avg.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------------------------------+\n",
      "|          Date_Time|concat_ws( , collect_list(Cleaned_Tweets))|\n",
      "+-------------------+------------------------------------------+\n",
      "|2017-11-09 09:00:00|                      Segwit2X died Thi...|\n",
      "|2017-11-04 14:00:00|                      NEW Roger Ver CEO...|\n",
      "|2017-11-04 13:00:00|                      Bitcoin prices me...|\n",
      "|2017-11-02 12:00:00|                      Nice Ethersport a...|\n",
      "|2017-11-07 23:00:00|                      CME Unveils Bitco...|\n",
      "+-------------------+------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql import functions as f\n",
    "# df_with_text = FinalTw.groupby(\"Date_Time\").agg(f.concat_ws(\" \", f.collect_list(FinalTw.Cleaned_Tweets)))\n",
    "# df_with_text.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FinalTw_avg.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+-------+\n",
      "|               date|              score|          Date_Time|  Price|\n",
      "+-------------------+-------------------+-------------------+-------+\n",
      "|2017-11-02 12:00:00|0.06527885267149504|2017-11-02 12:00:00|6975.91|\n",
      "|2017-11-04 13:00:00| 0.1172484553339806|2017-11-04 13:00:00|7313.96|\n",
      "|2017-11-04 14:00:00|0.11845594881100904|2017-11-04 14:00:00|7300.69|\n",
      "|2017-11-02 02:00:00|0.09436981377657502|2017-11-02 02:00:00|6814.19|\n",
      "|2017-11-04 09:00:00|0.09418897322089734|2017-11-04 09:00:00|7143.41|\n",
      "|2017-11-07 23:00:00|0.10283782658623176|2017-11-07 23:00:00|7112.74|\n",
      "|2017-11-03 03:00:00|0.09344412833299827|2017-11-03 03:00:00|7195.12|\n",
      "|2017-11-04 04:00:00|0.10142201882228671|2017-11-04 04:00:00|7255.67|\n",
      "|2017-10-31 17:00:00|0.09206832917391784|2017-10-31 17:00:00|6361.79|\n",
      "|2017-11-05 00:00:00|0.08632902932326081|2017-11-05 00:00:00|7343.74|\n",
      "|2017-11-03 05:00:00|0.07860593016294816|2017-11-03 05:00:00|7233.02|\n",
      "|2017-11-06 16:00:00|0.09368665402965974|2017-11-06 16:00:00|7155.73|\n",
      "|2017-11-02 21:00:00|0.08349927669926008|2017-11-02 21:00:00| 7083.6|\n",
      "|2017-11-06 19:00:00|0.10290178138846101|2017-11-06 19:00:00|7024.93|\n",
      "|2017-10-31 16:00:00|0.09301943334583278|2017-10-31 16:00:00|6364.78|\n",
      "|2017-11-01 05:00:00|0.11368575691864594|2017-11-01 05:00:00|6399.43|\n",
      "|2017-11-02 16:00:00|0.06061797001144715|2017-11-02 16:00:00|6965.58|\n",
      "|2017-10-31 12:00:00|0.10549754665746006|2017-10-31 12:00:00|6201.03|\n",
      "|2017-11-04 17:00:00| 0.1112281708270191|2017-11-04 17:00:00|7342.51|\n",
      "|2017-11-04 19:00:00|0.09140639081231433|2017-11-04 19:00:00|7422.15|\n",
      "+-------------------+-------------------+-------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql.functions import *\n",
    "# df_sort = FinalTw_avg.sort(asc(\"Date_Time\"))\n",
    "# df_sort.show()\n",
    "\n",
    "# temp1 = FinalTw_avg.alias('temp1')\n",
    "# temp2 = FinalBtc.alias('temp2')\n",
    "# FinalTwTemp = FinalTw_avg.limit(10)\n",
    "\n",
    "FinalTw_avg = FinalTw_avg.join(FinalBtc, FinalTw_avg.date == FinalBtc.Date_Time)\n",
    "FinalTw_avg = FinalTw_avg.selectExpr('Date_Time', 'score as Sentiment_score', 'Price')\n",
    "FinalTw_avg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`avgs.DateTime`' given input columns: [Date_Time, avg(Sentiment_score), Date_Time, Price]; line 1 pos 34;\\n'Sort ['avgs.DateTime ASC NULLS FIRST], true\\n+- 'Project [*]\\n   +- 'Join Inner, ('avgs.DateTime = Date_Time#131)\\n      :- SubqueryAlias avgs\\n      :  +- Aggregate [Date_Time#161], [Date_Time#161, avg(cast(Sentiment_score#65 as double)) AS avg(Sentiment_score)#181]\\n      :     +- Project [Date_Time#161, Sentiment_score#65]\\n      :        +- Project [UTC#155 AS Date_Time#161, Cleaned_Tweets#107, Sentiment_score#65]\\n      :           +- Project [Date_Time#150, Cleaned_Tweets#107, Sentiment_score#65, cast((unix_timestamp(Date_Time#150, yyyy-MM-dd HH:mm:ss, Some(America/New_York)) + cast(18000 as bigint)) as timestamp) AS UTC#155]\\n      :              +- Project [dt_truncated#144 AS Date_Time#150, Cleaned_Tweets#107, Sentiment_score#65]\\n      :                 +- Project [Date_Time#106, Cleaned_Tweets#107, Sentiment_score#65, cast((round((cast(unix_timestamp(Date_Time#106, yyyy-MM-dd HH:mm:ss, Some(America/New_York)) as double) / cast(3600 as double)), 0) * cast(3600 as double)) as timestamp) AS dt_truncated#144]\\n      :                    +- Project [DateTime_casted#86 AS Date_Time#106, CleanedTweets#54 AS Cleaned_Tweets#107, Sentiment_score#65]\\n      :                       +- Project [DateTime#37, Tweet#41, CleanedTweets#54, Sentiment_score#65, DateTime_c#79, cast(DateTime_c#79 as timestamp) AS DateTime_casted#86]\\n      :                          +- Project [DateTime#37, Tweet#41, CleanedTweets#54, Sentiment_score#65, Tw_Time_format(DateTime#37) AS DateTime_c#79]\\n      :                             +- Project [DateTime#37, Tweet#41, CleanedTweets#54, senti_score_udf(CleanedTweets#54) AS Sentiment_score#65]\\n      :                                +- Project [DateTime#37, Tweet#41, function_udf(Tweet#41) AS CleanedTweets#54]\\n      :                                   +- Project [DateTime#37, 1#1 AS Tweet#41]\\n      :                                      +- Project [0#0 AS DateTime#37, 1#1]\\n      :                                         +- Filter AtLeastNNulls(n, 0#0,1#1)\\n      :                                            +- LogicalRDD [0#0, 1#1]\\n      +- SubqueryAlias prices\\n         +- Project [Date_Time#131, cast(Price#49 as double) AS Price#135]\\n            +- Project [Cleaned_BTC_Time_New#125 AS Date_Time#131, Price#49]\\n               +- Project [DateTime#45, Price#49, Cleaned_BTC_Time#120, cast(Cleaned_BTC_Time#120 as timestamp) AS Cleaned_BTC_Time_New#125]\\n                  +- Project [DateTime#45, Price#49, Btc_Time_format(DateTime#45) AS Cleaned_BTC_Time#120]\\n                     +- Filter NOT (DateTime#45 = Date)\\n                        +- Project [DateTime#45, 1#6 AS Price#49]\\n                           +- Project [0#5 AS DateTime#45, 1#6]\\n                              +- LogicalRDD [0#5, 1#6]\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o19.sql.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`avgs.DateTime`' given input columns: [Date_Time, avg(Sentiment_score), Date_Time, Price]; line 1 pos 34;\n'Sort ['avgs.DateTime ASC NULLS FIRST], true\n+- 'Project [*]\n   +- 'Join Inner, ('avgs.DateTime = Date_Time#131)\n      :- SubqueryAlias avgs\n      :  +- Aggregate [Date_Time#161], [Date_Time#161, avg(cast(Sentiment_score#65 as double)) AS avg(Sentiment_score)#181]\n      :     +- Project [Date_Time#161, Sentiment_score#65]\n      :        +- Project [UTC#155 AS Date_Time#161, Cleaned_Tweets#107, Sentiment_score#65]\n      :           +- Project [Date_Time#150, Cleaned_Tweets#107, Sentiment_score#65, cast((unix_timestamp(Date_Time#150, yyyy-MM-dd HH:mm:ss, Some(America/New_York)) + cast(18000 as bigint)) as timestamp) AS UTC#155]\n      :              +- Project [dt_truncated#144 AS Date_Time#150, Cleaned_Tweets#107, Sentiment_score#65]\n      :                 +- Project [Date_Time#106, Cleaned_Tweets#107, Sentiment_score#65, cast((round((cast(unix_timestamp(Date_Time#106, yyyy-MM-dd HH:mm:ss, Some(America/New_York)) as double) / cast(3600 as double)), 0) * cast(3600 as double)) as timestamp) AS dt_truncated#144]\n      :                    +- Project [DateTime_casted#86 AS Date_Time#106, CleanedTweets#54 AS Cleaned_Tweets#107, Sentiment_score#65]\n      :                       +- Project [DateTime#37, Tweet#41, CleanedTweets#54, Sentiment_score#65, DateTime_c#79, cast(DateTime_c#79 as timestamp) AS DateTime_casted#86]\n      :                          +- Project [DateTime#37, Tweet#41, CleanedTweets#54, Sentiment_score#65, Tw_Time_format(DateTime#37) AS DateTime_c#79]\n      :                             +- Project [DateTime#37, Tweet#41, CleanedTweets#54, senti_score_udf(CleanedTweets#54) AS Sentiment_score#65]\n      :                                +- Project [DateTime#37, Tweet#41, function_udf(Tweet#41) AS CleanedTweets#54]\n      :                                   +- Project [DateTime#37, 1#1 AS Tweet#41]\n      :                                      +- Project [0#0 AS DateTime#37, 1#1]\n      :                                         +- Filter AtLeastNNulls(n, 0#0,1#1)\n      :                                            +- LogicalRDD [0#0, 1#1]\n      +- SubqueryAlias prices\n         +- Project [Date_Time#131, cast(Price#49 as double) AS Price#135]\n            +- Project [Cleaned_BTC_Time_New#125 AS Date_Time#131, Price#49]\n               +- Project [DateTime#45, Price#49, Cleaned_BTC_Time#120, cast(Cleaned_BTC_Time#120 as timestamp) AS Cleaned_BTC_Time_New#125]\n                  +- Project [DateTime#45, Price#49, Btc_Time_format(DateTime#45) AS Cleaned_BTC_Time#120]\n                     +- Filter NOT (DateTime#45 = Date)\n                        +- Project [DateTime#45, 1#6 AS Price#49]\n                           +- Project [0#5 AS DateTime#45, 1#6]\n                              +- LogicalRDD [0#5, 1#6]\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:279)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:289)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:290)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:298)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:298)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:268)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:78)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:78)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:623)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-60f3df27e81e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mFinalTw_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregisterTempTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"avgs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mFinalBtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregisterTempTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prices\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT * FROM avgs JOIN prices ON avgs.DateTime = prices.Date_Time order by avgs.DateTime\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/sql/context.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \"\"\"\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \"\"\"\n\u001b[0;32m--> 556\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`avgs.DateTime`' given input columns: [Date_Time, avg(Sentiment_score), Date_Time, Price]; line 1 pos 34;\\n'Sort ['avgs.DateTime ASC NULLS FIRST], true\\n+- 'Project [*]\\n   +- 'Join Inner, ('avgs.DateTime = Date_Time#131)\\n      :- SubqueryAlias avgs\\n      :  +- Aggregate [Date_Time#161], [Date_Time#161, avg(cast(Sentiment_score#65 as double)) AS avg(Sentiment_score)#181]\\n      :     +- Project [Date_Time#161, Sentiment_score#65]\\n      :        +- Project [UTC#155 AS Date_Time#161, Cleaned_Tweets#107, Sentiment_score#65]\\n      :           +- Project [Date_Time#150, Cleaned_Tweets#107, Sentiment_score#65, cast((unix_timestamp(Date_Time#150, yyyy-MM-dd HH:mm:ss, Some(America/New_York)) + cast(18000 as bigint)) as timestamp) AS UTC#155]\\n      :              +- Project [dt_truncated#144 AS Date_Time#150, Cleaned_Tweets#107, Sentiment_score#65]\\n      :                 +- Project [Date_Time#106, Cleaned_Tweets#107, Sentiment_score#65, cast((round((cast(unix_timestamp(Date_Time#106, yyyy-MM-dd HH:mm:ss, Some(America/New_York)) as double) / cast(3600 as double)), 0) * cast(3600 as double)) as timestamp) AS dt_truncated#144]\\n      :                    +- Project [DateTime_casted#86 AS Date_Time#106, CleanedTweets#54 AS Cleaned_Tweets#107, Sentiment_score#65]\\n      :                       +- Project [DateTime#37, Tweet#41, CleanedTweets#54, Sentiment_score#65, DateTime_c#79, cast(DateTime_c#79 as timestamp) AS DateTime_casted#86]\\n      :                          +- Project [DateTime#37, Tweet#41, CleanedTweets#54, Sentiment_score#65, Tw_Time_format(DateTime#37) AS DateTime_c#79]\\n      :                             +- Project [DateTime#37, Tweet#41, CleanedTweets#54, senti_score_udf(CleanedTweets#54) AS Sentiment_score#65]\\n      :                                +- Project [DateTime#37, Tweet#41, function_udf(Tweet#41) AS CleanedTweets#54]\\n      :                                   +- Project [DateTime#37, 1#1 AS Tweet#41]\\n      :                                      +- Project [0#0 AS DateTime#37, 1#1]\\n      :                                         +- Filter AtLeastNNulls(n, 0#0,1#1)\\n      :                                            +- LogicalRDD [0#0, 1#1]\\n      +- SubqueryAlias prices\\n         +- Project [Date_Time#131, cast(Price#49 as double) AS Price#135]\\n            +- Project [Cleaned_BTC_Time_New#125 AS Date_Time#131, Price#49]\\n               +- Project [DateTime#45, Price#49, Cleaned_BTC_Time#120, cast(Cleaned_BTC_Time#120 as timestamp) AS Cleaned_BTC_Time_New#125]\\n                  +- Project [DateTime#45, Price#49, Btc_Time_format(DateTime#45) AS Cleaned_BTC_Time#120]\\n                     +- Filter NOT (DateTime#45 = Date)\\n                        +- Project [DateTime#45, 1#6 AS Price#49]\\n                           +- Project [0#5 AS DateTime#45, 1#6]\\n                              +- LogicalRDD [0#5, 1#6]\\n\""
     ]
    }
   ],
   "source": [
    "# FinalTw_avg.registerTempTable(\"avgs\")\n",
    "# FinalBtc.registerTempTable(\"prices\")\n",
    "# results = sql.sql(\"SELECT * FROM avgs JOIN prices ON avgs.DateTime = prices.Date_Time order by avgs.DateTime\")\n",
    "# results.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FinalTw_avg.repartition(1).write.csv(\"One.csv\") #this will write df to single csv instead of writing diff csv acc to partitions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
