{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as spark\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import col,udf,monotonically_increasing_id,unix_timestamp,round,avg\n",
    "import re\n",
    "sc = spark.SparkContext()\n",
    "sql = spark.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping line 845142: unexpected end of data\n"
     ]
    }
   ],
   "source": [
    "df1=pd.read_csv('/Users/harishpuvvada/Desktop/PBDA/IPynb Spark/tweetsfinal.csv',error_bad_lines=False,engine = 'python',header = None) \n",
    "df2=pd.read_csv('/Users/harishpuvvada/Desktop/PBDA/IPynb Spark/BitCoinPrice.csv',error_bad_lines=False,engine = 'python',header = None) \n",
    "FullDataTw=sql.createDataFrame(df1)\n",
    "FullDataBtc=sql.createDataFrame(df2) #creating pandas df and then changing it to pyspark df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "845141\n",
      "217\n"
     ]
    }
   ],
   "source": [
    "FullDataTw = FullDataTw.dropna() #getting rid of full empty rows\n",
    "print(FullDataTw.count())\n",
    "print(FullDataBtc.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FullDataTw.select(monotonically_increasing_id().alias(\"rowId\"),\"*\")\n",
    "FullDataTw = FullDataTw.withColumnRenamed('0', 'DateTime') #setting column names of Twitter dataset\n",
    "FullDataTw = FullDataTw.withColumnRenamed('1', 'Tweet')\n",
    "FullDataBtc = FullDataBtc.withColumnRenamed('0', 'DateTime') #setting column names of Bitcoin price dataset\n",
    "FullDataBtc = FullDataBtc.withColumnRenamed('1', 'Price')\n",
    "FullDataBtc = FullDataBtc.filter(FullDataBtc.DateTime != 'Date') #to get rid of first row with the header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Tw_samp = FullDataTw #taking sample of 50 rows and working on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|            DateTime|               Tweet|       CleanedTweets|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|Thu Nov 09 17:43:...|RT @Forbes: The F...|The Failure of Se...|\n",
      "|Thu Nov 09 17:43:...|RT @mindstatex: L...|Lots of love from...|\n",
      "|Thu Nov 09 17:43:...|RT @FernandoHuama...|Warning Built in ...|\n",
      "|Thu Nov 09 17:43:...|RT @LevelNetwork:...|Join our telegram...|\n",
      "|Thu Nov 09 17:43:...|RT @realsheepwolf...|DIGAF FLOAT 16M T...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import preprocessor as p #cleaning each tweet using tweet-preprocessor like removing hashtags,urls,emojis....\n",
    "def function_udf(input_str):\n",
    "    input_str = re.sub(r'RT', '', input_str)\n",
    "    p.set_options(p.OPT.URL, p.OPT.EMOJI,p.OPT.MENTION)\n",
    "    input_str = p.clean(input_str)\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", input_str).split())\n",
    "func_udf = udf(function_udf, StringType())\n",
    "CleanDF = Tw_samp.withColumn('CleanedTweets', func_udf(Tw_samp['Tweet']))\n",
    "CleanDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+---------------+\n",
      "|            DateTime|               Tweet|       CleanedTweets|Sentiment_score|\n",
      "+--------------------+--------------------+--------------------+---------------+\n",
      "|Thu Nov 09 17:43:...|RT @Forbes: The F...|The Failure of Se...|    -0.18888889|\n",
      "|Thu Nov 09 17:43:...|RT @mindstatex: L...|Lots of love from...|     0.25833333|\n",
      "|Thu Nov 09 17:43:...|RT @FernandoHuama...|Warning Built in ...|            0.0|\n",
      "|Thu Nov 09 17:43:...|RT @LevelNetwork:...|Join our telegram...|            0.0|\n",
      "|Thu Nov 09 17:43:...|RT @realsheepwolf...|DIGAF FLOAT 16M T...|          -0.05|\n",
      "+--------------------+--------------------+--------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob  #passing cleaned tweets and getting a sentiment score for each tweet\n",
    "def senti_score_udf(input_str):\n",
    "    analysis = TextBlob(input_str)\n",
    "    return analysis.sentiment.polarity\n",
    "func_udf2 = udf(senti_score_udf, FloatType())\n",
    "CleanDF = CleanDF.withColumn('Sentiment_score', func_udf2(CleanDF['CleanedTweets']))\n",
    "CleanDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+---------------+-------------------+-------------------+\n",
      "|            DateTime|               Tweet|       CleanedTweets|Sentiment_score|         DateTime_c|    DateTime_casted|\n",
      "+--------------------+--------------------+--------------------+---------------+-------------------+-------------------+\n",
      "|Thu Nov 09 17:43:...|RT @Forbes: The F...|The Failure of Se...|    -0.18888889|2017-11-09 17:43:41|2017-11-09 17:43:41|\n",
      "|Thu Nov 09 17:43:...|RT @mindstatex: L...|Lots of love from...|     0.25833333|2017-11-09 17:43:40|2017-11-09 17:43:40|\n",
      "|Thu Nov 09 17:43:...|RT @FernandoHuama...|Warning Built in ...|            0.0|2017-11-09 17:43:39|2017-11-09 17:43:39|\n",
      "|Thu Nov 09 17:43:...|RT @LevelNetwork:...|Join our telegram...|            0.0|2017-11-09 17:43:39|2017-11-09 17:43:39|\n",
      "|Thu Nov 09 17:43:...|RT @realsheepwolf...|DIGAF FLOAT 16M T...|          -0.05|2017-11-09 17:43:39|2017-11-09 17:43:39|\n",
      "+--------------------+--------------------+--------------------+---------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def Tw_Time_format(stri):  #manipulating and casting the strings(DateTime) of tweets dataframe to timestamps\n",
    "    dic = {'Nov':'11','Oct':'10'}\n",
    "    ans = ''\n",
    "    ans += stri[-4:]+'-'+ dic[stri[4:7]]+'-'+stri[8:19]\n",
    "    return ans\n",
    "func_udf3 = udf(Tw_Time_format,StringType())\n",
    "CleanDF = CleanDF.withColumn('DateTime_c', func_udf3(CleanDF['DateTime']))\n",
    "CleanDF = CleanDF.withColumn(\"DateTime_casted\",CleanDF['DateTime_c'].cast(TimestampType()))\n",
    "CleanDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+---------------+\n",
      "|          Date_Time|      Cleaned_Tweets|Sentiment_score|\n",
      "+-------------------+--------------------+---------------+\n",
      "|2017-11-09 17:43:41|The Failure of Se...|    -0.18888889|\n",
      "|2017-11-09 17:43:40|Lots of love from...|     0.25833333|\n",
      "|2017-11-09 17:43:39|Warning Built in ...|            0.0|\n",
      "|2017-11-09 17:43:39|Join our telegram...|            0.0|\n",
      "|2017-11-09 17:43:39|DIGAF FLOAT 16M T...|          -0.05|\n",
      "+-------------------+--------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FinalTw = CleanDF.selectExpr(\"DateTime_casted as Date_Time\", \"CleanedTweets as Cleaned_Tweets\",\"Sentiment_score\")\n",
    "FinalTw.show(5) #selecting necessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date_Time: timestamp (nullable = true)\n",
      " |-- Cleaned_Tweets: string (nullable = true)\n",
      " |-- Sentiment_score: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FinalTw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime \n",
    "from dateutil import parser\n",
    "def Btc_Time_format(input_str): #manipulating and casting the strings(DateTime) of BTC dataframe to timestamps\n",
    "    input_str = re.sub(r'/17','', input_str)\n",
    "    input_str = '2017-'+ input_str\n",
    "    input_str = re.sub(r'/', '-', input_str)\n",
    "    input_str += ':00'\n",
    "    return input_str[:10]+\"\"+input_str[10:]\n",
    "func_udf = udf(Btc_Time_format, StringType())\n",
    "FullDataBtc = FullDataBtc.withColumn('Cleaned_BTC_Time', func_udf(FullDataBtc['DateTime']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+-------------------+\n",
      "|      DateTime|  Price|   Cleaned_BTC_Time|\n",
      "+--------------+-------+-------------------+\n",
      "| 10/31/17 0:00|6142.46| 2017-10-31 0:00:00|\n",
      "| 10/31/17 1:00|6139.47| 2017-10-31 1:00:00|\n",
      "| 10/31/17 2:00| 6128.2| 2017-10-31 2:00:00|\n",
      "| 10/31/17 3:00|6130.72| 2017-10-31 3:00:00|\n",
      "| 10/31/17 4:00|6143.92| 2017-10-31 4:00:00|\n",
      "| 10/31/17 5:00|6158.76| 2017-10-31 5:00:00|\n",
      "| 10/31/17 6:00| 6105.9| 2017-10-31 6:00:00|\n",
      "| 10/31/17 7:00|6094.36| 2017-10-31 7:00:00|\n",
      "| 10/31/17 8:00|6125.13| 2017-10-31 8:00:00|\n",
      "| 10/31/17 9:00|   6165| 2017-10-31 9:00:00|\n",
      "|10/31/17 10:00|6170.77|2017-10-31 10:00:00|\n",
      "|10/31/17 11:00|6233.74|2017-10-31 11:00:00|\n",
      "|10/31/17 12:00|6201.03|2017-10-31 12:00:00|\n",
      "|10/31/17 13:00|6332.34|2017-10-31 13:00:00|\n",
      "|10/31/17 14:00|6363.13|2017-10-31 14:00:00|\n",
      "|10/31/17 15:00|6365.16|2017-10-31 15:00:00|\n",
      "|10/31/17 16:00|6364.78|2017-10-31 16:00:00|\n",
      "|10/31/17 17:00|6361.79|2017-10-31 17:00:00|\n",
      "|10/31/17 18:00|6335.64|2017-10-31 18:00:00|\n",
      "|10/31/17 19:00|6341.15|2017-10-31 19:00:00|\n",
      "|10/31/17 20:00|6370.08|2017-10-31 20:00:00|\n",
      "|10/31/17 21:00|6393.26|2017-10-31 21:00:00|\n",
      "|10/31/17 22:00|6383.94|2017-10-31 22:00:00|\n",
      "|10/31/17 23:00|6448.36|2017-10-31 23:00:00|\n",
      "|  11/1/17 0:00|6376.49|  2017-11-1 0:00:00|\n",
      "|  11/1/17 1:00|6405.67|  2017-11-1 1:00:00|\n",
      "|  11/1/17 2:00| 6419.6|  2017-11-1 2:00:00|\n",
      "|  11/1/17 3:00|6426.88|  2017-11-1 3:00:00|\n",
      "|  11/1/17 4:00|6365.63|  2017-11-1 4:00:00|\n",
      "|  11/1/17 5:00|6399.43|  2017-11-1 5:00:00|\n",
      "+--------------+-------+-------------------+\n",
      "only showing top 30 rows\n",
      "\n",
      "+-------------------+-------+\n",
      "|          Date_Time|  Price|\n",
      "+-------------------+-------+\n",
      "|2017-10-31 00:00:00|6142.46|\n",
      "|2017-10-31 01:00:00|6139.47|\n",
      "|2017-10-31 02:00:00| 6128.2|\n",
      "|2017-10-31 03:00:00|6130.72|\n",
      "|2017-10-31 04:00:00|6143.92|\n",
      "|2017-10-31 05:00:00|6158.76|\n",
      "|2017-10-31 06:00:00| 6105.9|\n",
      "|2017-10-31 07:00:00|6094.36|\n",
      "|2017-10-31 08:00:00|6125.13|\n",
      "|2017-10-31 09:00:00| 6165.0|\n",
      "|2017-10-31 10:00:00|6170.77|\n",
      "|2017-10-31 11:00:00|6233.74|\n",
      "|2017-10-31 12:00:00|6201.03|\n",
      "|2017-10-31 13:00:00|6332.34|\n",
      "|2017-10-31 14:00:00|6363.13|\n",
      "|2017-10-31 15:00:00|6365.16|\n",
      "|2017-10-31 16:00:00|6364.78|\n",
      "|2017-10-31 17:00:00|6361.79|\n",
      "|2017-10-31 18:00:00|6335.64|\n",
      "|2017-10-31 19:00:00|6341.15|\n",
      "|2017-10-31 20:00:00|6370.08|\n",
      "|2017-10-31 21:00:00|6393.26|\n",
      "|2017-10-31 22:00:00|6383.94|\n",
      "|2017-10-31 23:00:00|6448.36|\n",
      "|2017-11-01 00:00:00|6376.49|\n",
      "|2017-11-01 01:00:00|6405.67|\n",
      "|2017-11-01 02:00:00| 6419.6|\n",
      "|2017-11-01 03:00:00|6426.88|\n",
      "|2017-11-01 04:00:00|6365.63|\n",
      "|2017-11-01 05:00:00|6399.43|\n",
      "|2017-11-01 06:00:00|6400.97|\n",
      "|2017-11-01 07:00:00| 6392.6|\n",
      "|2017-11-01 08:00:00|6446.61|\n",
      "|2017-11-01 09:00:00|6541.77|\n",
      "|2017-11-01 10:00:00|6584.45|\n",
      "|2017-11-01 11:00:00| 6568.0|\n",
      "|2017-11-01 12:00:00|6509.08|\n",
      "|2017-11-01 13:00:00|6484.66|\n",
      "|2017-11-01 14:00:00|6547.65|\n",
      "|2017-11-01 15:00:00|6556.74|\n",
      "+-------------------+-------+\n",
      "only showing top 40 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FullDataBtc.show(30)\n",
    "CleandfBtc = FullDataBtc.withColumn(\"Cleaned_BTC_Time_New\",FullDataBtc['Cleaned_BTC_Time'].cast(TimestampType()))\n",
    "FinalBtc = CleandfBtc.selectExpr(\"Cleaned_BTC_Time_New as Date_Time\", \"Price\")\n",
    "FinalBtc = FinalBtc.withColumn(\"Price\",FinalBtc['Price'].cast(DoubleType()))\n",
    "FinalBtc.show(40)#In this cell, casting to timesstamp, changing col names and casting price type to double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date_Time: timestamp (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FinalBtc.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+---------------+\n",
      "|          Date_Time|      Cleaned_Tweets|Sentiment_score|\n",
      "+-------------------+--------------------+---------------+\n",
      "|2017-11-09 23:00:00|The Failure of Se...|    -0.18888889|\n",
      "|2017-11-09 23:00:00|Lots of love from...|     0.25833333|\n",
      "|2017-11-09 23:00:00|Warning Built in ...|            0.0|\n",
      "|2017-11-09 23:00:00|Join our telegram...|            0.0|\n",
      "|2017-11-09 23:00:00|DIGAF FLOAT 16M T...|          -0.05|\n",
      "|2017-11-09 23:00:00|My luggage likes ...|            0.0|\n",
      "|2017-11-09 23:00:00|As Bitcoin become...|           0.55|\n",
      "|2017-11-09 23:00:00|A crucial feature...|            0.1|\n",
      "|2017-11-09 23:00:00|As Bitcoin become...|           0.55|\n",
      "|2017-11-09 23:00:00|As Bitcoin become...|           0.55|\n",
      "+-------------------+--------------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_truncated = ((round(unix_timestamp(col('Date_Time')) / 3600) * 3600).cast('timestamp'))\n",
    "FinalTw = FinalTw.withColumn('dt_truncated', dt_truncated)\n",
    "FinalTw = FinalTw.selectExpr(\"dt_truncated as Date_Time\",\"Cleaned_Tweets\",\"Sentiment_score\")\n",
    "UTC = ((unix_timestamp(col('Date_Time'))+ 5*60*60).cast('timestamp'))\n",
    "FinalTw = FinalTw.withColumn('UTC', UTC)\n",
    "FinalTw = FinalTw.selectExpr(\"UTC as Date_Time\",\"Cleaned_Tweets\",\"Sentiment_score\")\n",
    "FinalTw.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|           DateTime|    Sentiment_score|\n",
      "+-------------------+-------------------+\n",
      "|2017-11-09 09:00:00|0.09240741625240456|\n",
      "|2017-11-04 14:00:00|0.11845594881100904|\n",
      "|2017-11-04 13:00:00| 0.1172484553339806|\n",
      "|2017-11-02 12:00:00|0.06527885267149504|\n",
      "|2017-11-07 23:00:00|0.10283782658623176|\n",
      "|2017-11-04 09:00:00|0.09418897322089734|\n",
      "|2017-11-02 02:00:00|0.09436981377657502|\n",
      "|2017-11-03 03:00:00|0.09344412833299827|\n",
      "|2017-11-04 04:00:00|0.10142201882228671|\n",
      "|2017-11-05 00:00:00|0.08632902932326081|\n",
      "|2017-10-31 17:00:00|0.09206832917391784|\n",
      "|2017-11-06 16:00:00|0.09368665402965974|\n",
      "|2017-11-03 05:00:00|0.07860593016294816|\n",
      "|2017-11-02 21:00:00|0.08349927669926008|\n",
      "|2017-11-06 19:00:00|0.10290178138846101|\n",
      "|2017-11-02 16:00:00|0.06061797001144715|\n",
      "|2017-11-01 05:00:00|0.11368575691864594|\n",
      "|2017-10-31 16:00:00|0.09301943334583278|\n",
      "|2017-10-31 12:00:00|0.10549754665746006|\n",
      "|2017-11-04 19:00:00|0.09140639081231433|\n",
      "+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# FinalTw_avg = FinalTw.select(\"Date_Time\",\"Sentiment_score\").groupBy(\"Date_Time\").agg(avg(col(\"Sentiment_score\")))\n",
    "# FinalTw_avg.show()\n",
    "FinalTw.registerTempTable(\"temp\")\n",
    "FinalTw_avg = sql.sql(\"SELECT Date_Time As DateTime, AVG(Sentiment_score) As Sentiment_score FROM temp GROUP BY Date_Time\")\n",
    "FinalTw_avg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------------------------------+\n",
      "|          Date_Time|concat_ws( , collect_list(Cleaned_Tweets))|\n",
      "+-------------------+------------------------------------------+\n",
      "|2017-11-09 09:00:00|                      Segwit2X died Thi...|\n",
      "|2017-11-04 14:00:00|                      NEW Roger Ver CEO...|\n",
      "|2017-11-04 13:00:00|                      Bitcoin prices me...|\n",
      "|2017-11-02 12:00:00|                      Nice Ethersport a...|\n",
      "|2017-11-07 23:00:00|                      CME Unveils Bitco...|\n",
      "|2017-11-04 09:00:00|                      One Bitcoin Trans...|\n",
      "|2017-11-02 02:00:00|                      This is the first...|\n",
      "|2017-11-03 03:00:00|                      Coinbase Adds A R...|\n",
      "|2017-11-04 04:00:00|                      Kyle Bass Intervi...|\n",
      "|2017-11-05 00:00:00|                      Spec Check out th...|\n",
      "|2017-10-31 17:00:00|                      Hot crypto altcoi...|\n",
      "|2017-11-06 16:00:00|                      How to generate u...|\n",
      "|2017-11-03 05:00:00|                      bitcoin just hit ...|\n",
      "|2017-11-02 21:00:00|                      Bitcoin Is the Ve...|\n",
      "|2017-11-06 19:00:00|                      BTCUSD Ribasso Bi...|\n",
      "|2017-11-02 16:00:00|                      Bitcoin Exchange ...|\n",
      "|2017-11-01 05:00:00|                      Understanding Seg...|\n",
      "|2017-10-31 16:00:00|                      Bitcoin Surges Af...|\n",
      "|2017-10-31 12:00:00|                      CRYPTO BRANDS Bit...|\n",
      "|2017-11-04 19:00:00|                      Bitcoin 125 Billi...|\n",
      "+-------------------+------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as f\n",
    "df_with_text = FinalTw.groupby(\"Date_Time\").agg(f.concat_ws(\" \", f.collect_list(FinalTw.Cleaned_Tweets)))\n",
    "df_with_text.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FinalTw_avg.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|           DateTime|    Sentiment_score|\n",
      "+-------------------+-------------------+\n",
      "|2017-10-31 05:00:00|0.09226261767339003|\n",
      "|2017-10-31 06:00:00|0.10376996608248826|\n",
      "|2017-10-31 07:00:00|0.11169094251431187|\n",
      "|2017-10-31 08:00:00|0.08213433168664136|\n",
      "|2017-10-31 09:00:00|0.08937042968493204|\n",
      "|2017-10-31 10:00:00|0.10267176426801841|\n",
      "|2017-10-31 11:00:00|0.12814641170317434|\n",
      "|2017-10-31 12:00:00|0.10549754665746006|\n",
      "|2017-10-31 13:00:00|0.13845789734397101|\n",
      "|2017-10-31 14:00:00|0.11779438208175155|\n",
      "|2017-10-31 15:00:00|0.09131997304766154|\n",
      "|2017-10-31 16:00:00|0.09301943334583278|\n",
      "|2017-10-31 17:00:00|0.09206832917391784|\n",
      "|2017-10-31 18:00:00|0.10691324426615845|\n",
      "|2017-10-31 19:00:00|0.12217075088870974|\n",
      "|2017-10-31 20:00:00|0.12817142761147934|\n",
      "|2017-10-31 21:00:00|0.09238581351736849|\n",
      "|2017-10-31 22:00:00|0.10682700448924513|\n",
      "|2017-10-31 23:00:00|0.10207913246967248|\n",
      "|2017-11-01 00:00:00| 0.1003165565658219|\n",
      "+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql.functions import *\n",
    "# df_sort = FinalTw_avg.sort(asc(\"Date_Time\"))\n",
    "# df_sort.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+-------+\n",
      "|           DateTime|    Sentiment_score|          Date_Time|  Price|\n",
      "+-------------------+-------------------+-------------------+-------+\n",
      "|2017-10-31 05:00:00|0.09226261767339003|2017-10-31 05:00:00|6158.76|\n",
      "|2017-10-31 06:00:00|0.10376996608248826|2017-10-31 06:00:00| 6105.9|\n",
      "|2017-10-31 07:00:00|0.11169094251431187|2017-10-31 07:00:00|6094.36|\n",
      "|2017-10-31 08:00:00|0.08213433168664136|2017-10-31 08:00:00|6125.13|\n",
      "|2017-10-31 09:00:00|0.08937042968493204|2017-10-31 09:00:00| 6165.0|\n",
      "|2017-10-31 10:00:00|0.10267176426801841|2017-10-31 10:00:00|6170.77|\n",
      "|2017-10-31 11:00:00|0.12814641170317434|2017-10-31 11:00:00|6233.74|\n",
      "|2017-10-31 12:00:00|0.10549754665746006|2017-10-31 12:00:00|6201.03|\n",
      "|2017-10-31 13:00:00|0.13845789734397101|2017-10-31 13:00:00|6332.34|\n",
      "|2017-10-31 14:00:00|0.11779438208175155|2017-10-31 14:00:00|6363.13|\n",
      "|2017-10-31 15:00:00|0.09131997304766154|2017-10-31 15:00:00|6365.16|\n",
      "|2017-10-31 16:00:00|0.09301943334583278|2017-10-31 16:00:00|6364.78|\n",
      "|2017-10-31 17:00:00|0.09206832917391784|2017-10-31 17:00:00|6361.79|\n",
      "|2017-10-31 18:00:00|0.10691324426615845|2017-10-31 18:00:00|6335.64|\n",
      "|2017-10-31 19:00:00|0.12217075088870974|2017-10-31 19:00:00|6341.15|\n",
      "|2017-10-31 20:00:00|0.12817142761147934|2017-10-31 20:00:00|6370.08|\n",
      "|2017-10-31 21:00:00|0.09238581351736849|2017-10-31 21:00:00|6393.26|\n",
      "|2017-10-31 22:00:00|0.10682700448924513|2017-10-31 22:00:00|6383.94|\n",
      "|2017-10-31 23:00:00|0.10207913246967248|2017-10-31 23:00:00|6448.36|\n",
      "|2017-11-01 00:00:00| 0.1003165565658219|2017-11-01 00:00:00|6376.49|\n",
      "|2017-11-01 01:00:00|0.09925359364187578|2017-11-01 01:00:00|6405.67|\n",
      "|2017-11-01 02:00:00| 0.0903783471957929|2017-11-01 02:00:00| 6419.6|\n",
      "|2017-11-01 03:00:00|0.08084288445755294|2017-11-01 03:00:00|6426.88|\n",
      "|2017-11-01 04:00:00| 0.0952848910172815|2017-11-01 04:00:00|6365.63|\n",
      "|2017-11-01 05:00:00|0.11368575691864594|2017-11-01 05:00:00|6399.43|\n",
      "|2017-11-01 06:00:00|0.09042962332897247|2017-11-01 06:00:00|6400.97|\n",
      "|2017-11-01 07:00:00|0.10544019936526403|2017-11-01 07:00:00| 6392.6|\n",
      "|2017-11-01 08:00:00|0.10044294823240624|2017-11-01 08:00:00|6446.61|\n",
      "|2017-11-01 09:00:00|0.08704017784707618|2017-11-01 09:00:00|6541.77|\n",
      "|2017-11-01 10:00:00|0.09483974379779264|2017-11-01 10:00:00|6584.45|\n",
      "|2017-11-01 11:00:00|0.11055747954216864|2017-11-01 11:00:00| 6568.0|\n",
      "|2017-11-01 12:00:00|0.08590353839574041|2017-11-01 12:00:00|6509.08|\n",
      "|2017-11-01 13:00:00|0.10319565205908404|2017-11-01 13:00:00|6484.66|\n",
      "|2017-11-01 14:00:00|0.10654713353890957|2017-11-01 14:00:00|6547.65|\n",
      "|2017-11-01 15:00:00|0.09933015033777949|2017-11-01 15:00:00|6556.74|\n",
      "|2017-11-01 16:00:00|0.11030915142668184|2017-11-01 16:00:00|6587.46|\n",
      "|2017-11-01 17:00:00|0.09998502082288237|2017-11-01 17:00:00|6604.57|\n",
      "|2017-11-01 18:00:00|0.11224797952609222|2017-11-01 18:00:00|6585.55|\n",
      "|2017-11-01 19:00:00|0.08886486497870702|2017-11-01 19:00:00|6615.47|\n",
      "|2017-11-01 20:00:00|0.08552363872418746|2017-11-01 20:00:00|6568.12|\n",
      "+-------------------+-------------------+-------------------+-------+\n",
      "only showing top 40 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FinalTw_avg.registerTempTable(\"avgs\")\n",
    "FinalBtc.registerTempTable(\"prices\")\n",
    "results = sql.sql(\"SELECT * FROM avgs JOIN prices ON avgs.DateTime = prices.Date_Time order by avgs.DateTime\")\n",
    "results.show(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.repartition(1).write.csv(\"Six.csv\") #this will write df to single csv instead of writing diff csv acc to partitions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
